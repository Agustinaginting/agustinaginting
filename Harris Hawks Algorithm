#import library
! pip install pyMetaheuristic
! pip install tqdm
! pip install PyPortfolioOpt
! pip install solution

#import data saham Lq45
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
# df = pd.read_csv('drive/MyDrive/Colab Notebooks/lq45_join.csv')
df = pd.read_csv('drive/MyDrive/Crawling/lq45_join.csv')
from google.colab import drive
drive.mount('/content/drive')

#Data Preparation
company = list(dict.fromkeys(df['company'].tolist()))
company.sort()

diff = list()
day = list()
rate = list()
for i in range(len(company)):
  df_1 = df[df['company']==company[i]]
  for j in range(len(df_1)):
    day.append(j)
    if(j==0):
      diff.append(0)
      rate.append(0)
    else:
      diff.append(df_1.Close.iloc[j]-df_1.Close.iloc[j-1])
      rate.append((df_1.Close.iloc[j]-df_1.Close.iloc[j-1])/df_1.Close.iloc[j])
  # close = df_1['Close'].tolist()

  column_name = company[0]
df[df['company']== column_name]['Date'].tolist()

#Data diubah atau digrupkan berdasarkan tanggal
date_list = df.groupby('Date').count().reset_index().Date.to_list()
date_list

#Data diubah sesuai jenis saham
from tqdm import tqdm
dlist = list()
for i in range(len(date_list)):
  d = dict()
  d['Date'] = date_list[i]
  print(date_list[i])
  for j in tqdm(range(len(company))):
    try:
      d[company[j]] = df[(df['Date']==date_list[i]) & (df['company']==company[j])].Close.iloc[0]
    except:
      d[company[j]] = None
  dlist.append(d)
  # df[(df['Date']==date_list[0]) & (df['company']==company[0])].Close[0]

  #Proses Mean Variance Optimization
  df_2 = pd.read_csv('drive/MyDrive/Crawling/lq_45_h.csv')
  new_df_2 = df_2.loc[1364:].reset_index(drop=True).copy()
new_df_2 = new_df_2[:-1]
new_df_2 = new_df_2.set_index('Date')

#mengambil data sebagai data test
test = ['ADRO','ARTO','MEDC','GOTO','BBCA']
new_df_2[test]

#Hitung covarians
from pypfopt.expected_returns import mean_historical_return
from pypfopt.risk_models import CovarianceShrinkage


mu = mean_historical_return(new_df_2[test])
S = CovarianceShrinkage(new_df_2[test]).ledoit_wolf()

#menghitung efficient dan weight saham
from pypfopt.efficient_frontier import EfficientFrontier

ef = EfficientFrontier(mu, S)
weights = ef.max_sharpe()

cleaned_weights = ef.clean_weights()
print(dict(cleaned_weights))
weights

#menghitung performance portofolio
ef.portfolio_performance(verbose=True)

#menghitung alokasi 
from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices

latest_prices = get_latest_prices(new_df_2)

da = DiscreteAllocation(weights, latest_prices, total_portfolio_value=500000000)

allocation, leftover = da.greedy_portfolio()
print("Discrete allocation:", allocation)
print("Funds remaining: Rp{:.2f}".format(leftover))

#Proses HHO
new_df_2
! pip install solution

import random
import numpy
import math
# from solution import solution
import time

def HHO(objf, lb, ub, dim, SearchAgents_no, Max_iter):

    #dim=30
    #SearchAgents_no=50
    #lb=-100
    #ub=100
    #Max_iter=500
        
    
    # initialize the location and Energy of the rabbit
    Rabbit_Location=numpy.zeros(dim)
    Rabbit_Energy=float("inf")  #change this to -inf for maximization problems
    
    if not isinstance(lb, list):
        lb = [lb for _ in range(dim)]
        ub = [ub for _ in range(dim)]
    lb = numpy.asarray(lb)
    ub = numpy.asarray(ub)
         
    #Initialize the locations of Harris' hawks
    X=numpy.asarray([x*(ub-lb)+lb for x in numpy.random.uniform(0,1,(SearchAgents_no, dim))])
    
    #Initialize convergence
    convergence_curve=numpy.zeros(Max_iter)
    
    
    ############################
    # s=solution()
    d = dict()
    print("HHO is now tackling")    

    timerStart=time.time() 
    # s.startTime=time.strftime("%Y-%m-%d-%H-%M-%S")
    ############################
    
    t=0  # Loop counter
    
    # Main loop
    while t<Max_iter:
        for i in range(0,SearchAgents_no):
            
            # Check boundries
                      
            X[i,:]=numpy.clip(X[i,:], lb, ub)
            
            # fitness of locations
            fitness=objf(X[i,:])
            
            # Update the location of Rabbit
            if fitness<Rabbit_Energy: # Change this to > for maximization problem
                Rabbit_Energy=fitness 
                Rabbit_Location=X[i,:].copy() 
            
        E1=2*(1-(t/Max_iter)) # factor to show the decreaing energy of rabbit    
        
        # Update the location of Harris' hawks 
        for i in range(0,SearchAgents_no):

            E0=2*random.random()-1  # -1<E0<1
            Escaping_Energy=E1*(E0)  # escaping energy of rabbit Eq. (3) in the paper

            # -------- Exploration phase Eq. (1) in paper -------------------

            if abs(Escaping_Energy)>=1:
                #Harris' hawks perch randomly based on 2 strategy:
                q = random.random()
                rand_Hawk_index = math.floor(SearchAgents_no*random.random())
                X_rand = X[rand_Hawk_index, :]
                if q<0.5:
                    # perch based on other family members
                    X[i,:]=X_rand-random.random()*abs(X_rand-2*random.random()*X[i,:])

                elif q>=0.5:
                    #perch on a random tall tree (random site inside group's home range)
                    X[i,:]=(Rabbit_Location - X.mean(0))-random.random()*((ub-lb)*random.random()+lb)

            # -------- Exploitation phase -------------------
            elif abs(Escaping_Energy)<1:
                #Attacking the rabbit using 4 strategies regarding the behavior of the rabbit

                #phase 1: ----- surprise pounce (seven kills) ----------
                #surprise pounce (seven kills): multiple, short rapid dives by different hawks

                r=random.random() # probablity of each event
                
                if r>=0.5 and abs(Escaping_Energy)<0.5: # Hard besiege Eq. (6) in paper
                    X[i,:]=(Rabbit_Location)-Escaping_Energy*abs(Rabbit_Location-X[i,:])

                if r>=0.5 and abs(Escaping_Energy)>=0.5:  # Soft besiege Eq. (4) in paper
                    Jump_strength=2*(1- random.random()) # random jump strength of the rabbit
                    X[i,:]=(Rabbit_Location-X[i,:])-Escaping_Energy*abs(Jump_strength*Rabbit_Location-X[i,:])
                
                #phase 2: --------performing team rapid dives (leapfrog movements)----------

                if r<0.5 and abs(Escaping_Energy)>=0.5: # Soft besiege Eq. (10) in paper
                    #rabbit try to escape by many zigzag deceptive motions
                    Jump_strength=2*(1-random.random())
                    X1=Rabbit_Location-Escaping_Energy*abs(Jump_strength*Rabbit_Location-X[i,:])
                    X1 = numpy.clip(X1, lb, ub)

                    if objf(X1)< fitness: # improved move?
                        X[i,:] = X1.copy()
                    else: # hawks perform levy-based short rapid dives around the rabbit
                        X2=Rabbit_Location-Escaping_Energy*abs(Jump_strength*Rabbit_Location-X[i,:])+numpy.multiply(numpy.random.randn(dim),Levy(dim))
                        X2 = numpy.clip(X2, lb, ub)
                        if objf(X2)< fitness:
                            X[i,:] = X2.copy()
                if r<0.5 and abs(Escaping_Energy)<0.5:   # Hard besiege Eq. (11) in paper
                     Jump_strength=2*(1-random.random())
                     X1=Rabbit_Location-Escaping_Energy*abs(Jump_strength*Rabbit_Location-X.mean(0))
                     X1 = numpy.clip(X1, lb, ub)
                     
                     if objf(X1)< fitness: # improved move?
                        X[i,:] = X1.copy()
                     else: # Perform levy-based short rapid dives around the rabbit
                         X2=Rabbit_Location-Escaping_Energy*abs(Jump_strength*Rabbit_Location-X.mean(0))+numpy.multiply(numpy.random.randn(dim),Levy(dim))
                         X2 = numpy.clip(X2, lb, ub)
                         if objf(X2)< fitness:
                            X[i,:] = X2.copy()
                
        convergence_curve[t]=Rabbit_Energy
        if (t%1==0):
               print(['At iteration '+ str(t)+ ' the best fitness is '+ str(Rabbit_Energy)])
        t=t+1
    
    timerEnd=time.time()  
    # s.endTime=time.strftime("%Y-%m-%d-%H-%M-%S")
    # s.executionTime=timerEnd-timerStart
    # s.convergence=convergence_curve
    # s.optimizer="HHO"   
    # s.objfname=objf.__name__
    # s.best =Rabbit_Energy 
    # s.bestIndividual = Rabbit_Location
    
    
    d['convergence']=convergence_curve
    d['optimizer']="HHO"   
    d['objfname']=objf.__name__
    d['best'] =Rabbit_Energy 
    d['bestIndividual'] = Rabbit_Location

    return d
    d

def Levy(dim):
    beta=1.5
    sigma=(math.gamma(1+beta)*math.sin(math.pi*beta/2)/(math.gamma((1+beta)/2)*beta*2**((beta-1)/2)))**(1/beta) 
    u= 0.01*numpy.random.randn(dim)*sigma
    v = numpy.random.randn(dim)
    zz = numpy.power(numpy.absolute(v),(1/beta))
    step = numpy.divide(u,zz)
    return step

random.random()
len(X)

#search Agent
import numpy as np
X = list()
for it in range(SearchAgents_no):
  left = 1.0
  x = list()
  for i in range(dim):
    if(i<4):
      x1 = random.uniform(0,left)
      x.append(x1)
      left = left-x1
    else:
      x.append(left)
  X.append(x)
X

easom(X[i,:])

Levy(10)
x1,x2,x3,x4,x5 = [0,0.5,0,0.1,0]

x1*mu[0] + x2*mu[1] + x3*mu[2] + x4*mu[3] + x5*mu[4]

X[i,:]=numpy.clip(X[i,:], lb, ub)
X[i,:]

#Fitness Value
fitness=return_value(mu,X[i,:])
fitness

# Update the location of Rabbit
if fitness>Rabbit_Energy: # Change this to > for maximization problem
    Rabbit_Energy=fitness 
    Rabbit_Location=X[i,:].copy() 
fitness>Rabbit_Energy

#energi mangsa 1
E1=2*(1-(t/Max_iter))
E1

# Iterasi
X=numpy.asarray([x*(ub-lb)+lb for x in numpy.random.uniform(0,1,(SearchAgents_no, dim))])
len(X)

#Function Value
def return_value(mu, variables_values = [0,0,0,0,0]):
  print(variables_values)
  x1,x2,x3,x4,x5 = variables_values
  func_value = x1*mu[0] + x2*mu[1] + x3*mu[2] + x4*mu[3] + x5*mu[4]
  return func_value
  
  #Variance
  def calculate_var(weights,sample_cov):
  variance = 0
  for i in range(len(new_df_2.columns)):
    for j in range(len(new_df_2.columns)):
      variance += weights[i]*weights[j]*sample_cov[i][j]
  print(variance)
  return variance
  
  d
  
  #Fitness value terbaik
  d['bestIndividual']-0.1
  
  #weight fitness value terbaik\
  weight = d['bestIndividual'] 
weight[0]*mu[0]+  weight[2]*mu[2]+ weight[4]*mu[4]

mu

# Required Libraries
import numpy as np

# HHO
from pyMetaheuristic.algorithm import harris_hawks_optimization
from pyMetaheuristic.utils import graphs
from pypfopt.expected_returns import mean_historical_return
from pypfopt.risk_models import CovarianceShrinkage


mu = mean_historical_return(new_df_2)
S = CovarianceShrinkage(new_df_2).ledoit_wolf()

# Target Function - It can be any function that needs to be minimize, However it has to have only one argument: 'variables_values'. This Argument must be a list of variables.
# For Instance, suppose that our Target Function is the Easom Function (With two variables x1 and x2. Global Minimum f(x1, x2) = -1 for, x1 = 3.14 and x2 = 3.14)

# Target Function: Easom Function
def easom(variables_values = [0,0]):
    x1, x2     = variables_values
    func_value = -np.cos(x1) * np.cos(x2) * np.exp(-(x1 - np.pi) ** 2 - (x2 - np.pi) ** 2)
    return func_value

def return_value(variables_values = [0,0]):
  x1,x2 = variables_values
  func_value = mu[0]+x1* + x2*mu[1]
  return func_value
  
  mu[0]
  
  
# HHO - Parameters
parameters = {
    'hawks': 2500,
    'min_values': (-5, -5),
    'max_values': (5, 5),
    'iterations': 100,
    'verbose': True
}
# HHO - Algorithm
hho = harris_hawks_optimization(target_function = easom, **parameters)

# HHO - Solution
variables = hho[:-1]
minimum   = hho[ -1]
print('Variables: ', np.around(variables, 4) , ' Minimum Value Found: ', round(minimum, 4) )


# HHO - Plot Solution
plot_parameters = {
    'min_values': (-5, -5),
    'max_values': (5, 5),
    'step': (0.1, 0.1),
    'solution': [variables],
    'proj_view': '3D',
    'view': 'notebook'
}
graphs.plot_single_function(target_function = easom, **plot_parameters)







